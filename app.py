"""
Gradio Workout Logger + ä½ çš„æ•™ç·´ï¼ˆGroqï¼‰â€” å–®æª”å¯åŸ·è¡Œ app.pyï¼ˆé›²ç«¯ä¿®æ­£ç‰ˆï¼‰
é‡é»æ›´æ–°ï¼š
- ç›´æ¥ä½¿ç”¨ Google Sheet åšè³‡æ–™ä¾†æºèˆ‡é¡¯ç¤ºä¾†æºï¼ˆ<cloud record>ï¼‰ã€‚
- è‡ªå‹•åµæ¸¬åˆ†é åç¨±ï¼šå„ªå…ˆ `SHEET_TITLE`ï¼ˆç’°å¢ƒè®Šæ•¸ï¼‰â†’ `records` â†’ `record` â†’ ç¬¬ä¸€å€‹åˆ†é ã€‚
- UI é¡¯ç¤º Cloud é€£ç·šç‹€æ…‹ã€ç›®æ¨™åˆ†é åç¨±èˆ‡ç›®å‰è¡Œæ•¸ï¼›Save å¾Œè¨Šæ¯æœƒé¡¯ç¤ºé›²ç«¯æ˜¯å¦æˆåŠŸèˆ‡ç¸½åˆ—æ•¸ã€‚
- 10 åˆ†é˜å…§åŒæ—¥æœŸ+åŒ item è¦†å¯«ã€å…§å®¹ç›¸åŒä¸å†é‡å­˜ï¼ˆä¸¦æš«æ™‚åœç”¨ Saveï¼‰ã€‚

åŸ·è¡Œï¼š
    pip install -r requirements.txt  # æˆ–ç›´æ¥ pip install gradio pandas python-dateutil groq gspread gspread_dataframe google-auth google-auth-oauthlib
    python app.py
ç’°å¢ƒè®Šæ•¸ï¼š
    groq_key=...                 # Groq API Key
    gspread_service_json=...     # è²¼æ•´æ®µ Service Account JSONï¼ˆæˆ–ä½¿ç”¨ GOOGLE_APPLICATION_CREDENTIALS æŒ‡å‘æª”æ¡ˆï¼‰
    SHEET_TITLE=records          # å¯é¸ï¼ŒæŒ‡å®šè¦ç”¨çš„ worksheet åç¨±
"""
from __future__ import annotations
import os, json, hashlib
from pathlib import Path
from typing import List, Optional, Tuple
from datetime import datetime, date, timedelta

# ---- ä¾éœ€æ±‚ï¼šgroq å®‰è£/åŒ¯å…¥ ----
try:
    from groq import Groq
except ImportError:
    os.system('pip install groq')
    from groq import Groq

# ---- Google Sheets ç›¸ä¾ ----
try:
    import gspread
    from gspread_dataframe import set_with_dataframe, get_as_dataframe
except ImportError:
    os.system('pip install gspread gspread_dataframe google-auth google-auth-oauthlib')
    import gspread
    from gspread_dataframe import set_with_dataframe, get_as_dataframe

import gradio as gr
import pandas as pd

# ------------ å¸¸æ•¸èˆ‡æª”æ¡ˆè·¯å¾‘ ------------
APP_TITLE = "Workout Logger"
RECORDS_CSV = Path("workout_records.csv")  # æœ¬åœ°å‚™æ´
ITEMS_JSON = Path("known_items.json")
NUM_SETS = 5
WINDOW_MINUTES = 10      # 10 åˆ†é˜å…§å¯è¦†å¯«
SHEET_ID = "1qWH-FQKqAMLXdN2uV4fcLIk5URRjBwY7nELznZ352og"
SHEET_TITLE_ENV = os.getenv("SHEET_TITLE", "records")  # å¯ç”¨ç’°å¢ƒè®Šæ•¸è¦†å¯«

# ------------ Groqï¼ˆæ•™ç·´æ©Ÿå™¨äººï¼‰è¨­å®š ------------
GROQ_API_KEY = os.getenv("groq_key")
try:
    groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
except Exception:
    groq_client = None

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€å€‹è¬›ç¹é«”ä¸­æ–‡(Zh-tw)çš„å¥èº«æ•™ç·´ï¼Œä½ å¾ˆæ¨‚è§€ã€æœƒé¼“å‹µäººï¼Œä¹Ÿæœƒè¬›æœ‰è¶£çš„ç¬‘è©±ã€‚"
    "ç„¡è«–å­¸ç”Ÿå•ä»€éº¼å•é¡Œï¼Œéƒ½ç›¡é‡æŠŠè©±é¡Œå¼•å°è‡³é‹å‹•èˆ‡å¥èº«ã€‚è«‹ç”¨å£èªã€çŸ­æ®µè½ï¼Œ"
    "æä¾›å…·é«”å¯è¡Œçš„è¨“ç·´å»ºè­°ï¼ˆå‹•ä½œ/çµ„æ•¸/é‡é‡æˆ–RPEï¼‰ï¼Œä¸¦é©åº¦æé†’å®‰å…¨èˆ‡æš–èº«æ”¾é¬†ã€‚"
)
GROQ_MODEL = "llama-3.3-70b-versatile"

# è¿½è¹¤é›²ç«¯ç‹€æ…‹
CLOUD_LAST_ERROR = ""
CLOUD_WS_TITLE = None

# ------------ Google Sheets å·¥å…· ------------

def _gs_client() -> Optional[gspread.Client]:
    try:
        sa_json = os.getenv("gspread_service_json") or os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
        if sa_json:
            creds_dict = json.loads(sa_json)
            return gspread.service_account_from_dict(creds_dict)
        return gspread.service_account()  # èµ° GOOGLE_APPLICATION_CREDENTIALS
    except Exception:
        return None


def _get_target_ws(sh: gspread.Spreadsheet) -> gspread.Worksheet:
    """
    ç›®æ¨™ worksheet æ±ºç­–ï¼š
    1) SHEET_TITLE_ENV
    2) 'records'
    3) 'record'
    4) ç¬¬ä¸€å€‹ç¾æœ‰åˆ†é 
    è‹¥éƒ½æ²’æœ‰ï¼Œå»ºç«‹ SHEET_TITLE_ENVã€‚
    """
    global CLOUD_WS_TITLE
    # å…ˆå˜—è©¦ç›´æ¥å–
    preferred = [SHEET_TITLE_ENV, "records", "record"]
    titles = [ws.title for ws in sh.worksheets()]
    for name in preferred:
        if name in titles:
            CLOUD_WS_TITLE = name
            return sh.worksheet(name)
    # æ²’æ‰¾åˆ°å°±ç”¨ç¬¬ä¸€å€‹
    if titles:
        CLOUD_WS_TITLE = titles[0]
        return sh.worksheet(titles[0])
    # è‹¥ç«Ÿç„¶æ²’æœ‰åˆ†é ï¼Œå»ºç«‹ä¸€å€‹
    CLOUD_WS_TITLE = SHEET_TITLE_ENV
    return sh.add_worksheet(title=SHEET_TITLE_ENV, rows=1000, cols=30)


def _open_or_create_ws(client: gspread.Client):
    sh = client.open_by_key(SHEET_ID)
    ws = _get_target_ws(sh)
    ensure_records_header(ws)
    return ws


def ensure_records_header(ws):
    cols = ["date", "item"]
    for s in range(1, NUM_SETS+1):
        cols += [f"set{s}_kg", f"set{s}_reps"]
    cols += ["note", "total_volume_kg", "created_at"]
    try:
        first_row = ws.row_values(1)
    except Exception:
        first_row = []
    if first_row != cols:
        ws.clear()
        ws.update([cols])


def read_cloud_df() -> Optional[pd.DataFrame]:
    """æ”¹ç”¨ get_all_values è®€å–ï¼Œé¿å… gspread_dataframe é€ æˆçš„ç©ºç™½åˆ—å•é¡Œã€‚"""
    global CLOUD_LAST_ERROR
    client = _gs_client()
    if not client:
        CLOUD_LAST_ERROR = "ç„¡æ³•å»ºç«‹ Google æ†‘è­‰ï¼ˆæœªè¨­å®š service account æˆ–æª”æ¡ˆè·¯å¾‘ï¼‰ã€‚"
        return None
    try:
        ws = _open_or_create_ws(client)
        rows = ws.get_all_values()  # 2D list
        if not rows:
            return None
        header = rows[0] if rows else []
        data = rows[1:] if len(rows) > 1 else []
        if not header:
            return None
        if not data:
            # å›å‚³ç©º DF ä½†ä¿ç•™æ¬„ä½
            df = pd.DataFrame(columns=header)
        else:
            df = pd.DataFrame(data, columns=header)
        # å°æ•¸å€¼æ¬„å˜—è©¦è½‰å‹ï¼Œç©ºç™½ä¿æŒç©ºå­—ä¸²
        for s in range(1, NUM_SETS+1):
            for sub in ("kg", "reps"):
                col = f"set{s}_{sub}"
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce")
        if "total_volume_kg" in df.columns:
            df["total_volume_kg"] = pd.to_numeric(df["total_volume_kg"], errors="coerce")
        CLOUD_LAST_ERROR = ""
        return df
    except Exception as e:
        CLOUD_LAST_ERROR = f"è®€å–é›²ç«¯å¤±æ•—ï¼š{e}"
        return None
    try:
        ws = _open_or_create_ws(client)
        df = get_as_dataframe(ws, evaluate_formulas=True, header=0)
        df = df.dropna(how='all')
        if df.empty:
            cols = ["date", "item"]
            for s in range(1, NUM_SETS+1):
                cols += [f"set{s}_kg", f"set{s}_reps"]
            cols += ["note", "total_volume_kg", "created_at"]
            df = pd.DataFrame(columns=cols)
        CLOUD_LAST_ERROR = ""
        return df
    except Exception as e:
        CLOUD_LAST_ERROR = f"è®€å–é›²ç«¯å¤±æ•—ï¼š{e}"
        return None


def write_cloud_df(df: pd.DataFrame) -> Tuple[bool, int]:
    """ä¸ç”¨ gspread_dataframeï¼Œç›´æ¥ ws.update(range_name='A1', values=...).
    å¦å¤–å¼·åˆ¶æ¬„ä½é †åºï¼Œä¸¦æŠŠ NaN è½‰ç‚ºç©ºå­—ä¸²ï¼Œé¿å…æ•´åˆ—è¢«è¦–ç‚ºç©ºç™½ã€‚"""
    global CLOUD_LAST_ERROR
    client = _gs_client()
    if not client:
        CLOUD_LAST_ERROR = "ç„¡æ³•å»ºç«‹ Google æ†‘è­‰ï¼ˆæœªè¨­å®š service account æˆ–æª”æ¡ˆè·¯å¾‘ï¼‰ã€‚"
        return False, 0
    try:
        ws = _open_or_create_ws(client)
        # æ¬„ä½é †åº
        cols = ["date", "item"] + sum(([f"set{s}_kg", f"set{s}_reps"] for s in range(1, NUM_SETS+1)), []) + ["note", "total_volume_kg", "created_at"]
        out_df = df.copy()
        # è‹¥ç¼ºæ¬„ä½è£œç©ºã€ä¸¦é‡æ’
        for c in cols:
            if c not in out_df.columns:
                out_df[c] = ""
        out_df = out_df[cols]
        out_df = out_df.fillna("")
        # è½‰æˆç´” Python åŸºæœ¬å‹åˆ¥
        raw_values = out_df.values.tolist()
        values: list[list] = []
        for row in raw_values:
            new_row = []
            for x in row:
                if isinstance(x, (int, float, str)):
                    new_row.append(x)
                else:
                    new_row.append(str(x) if x is not None else "")
            values.append(new_row)
        # æ¸…ç©º+å¯«å…¥
        ws.clear()
        ws.update(range_name="A1", values=[cols] + values)
        # èª¿æ•´å¤§å°
        try:
            ws.resize(rows=max(2, len(values) + 1), cols=len(cols))
        except Exception:
            pass
        CLOUD_LAST_ERROR = ""
        return True, len(values)
    except Exception as e:
        CLOUD_LAST_ERROR = f"å¯«å…¥é›²ç«¯å¤±æ•—ï¼š{e}"
        return False, 0
    try:
        ws = _open_or_create_ws(client)
        # æº–å‚™è³‡æ–™ï¼šå°‡ NaN è½‰æˆç©ºå­—ä¸²ï¼Œç¢ºä¿æœƒå¯«å‡ºåˆ—
        out_df = df.copy()
        out_df = out_df.fillna("")
        header = list(out_df.columns)
        values = out_df.values.tolist()
        ws.clear()
        ws.update("A1", [header] + values)
        # æœ€å¾Œèª¿æ•´è¡¨æ ¼å¤§å°
        try:
            ws.resize(rows=max(2, len(values) + 1), cols=len(header))
        except Exception:
            pass
        CLOUD_LAST_ERROR = ""
        return True, len(values)
    except Exception as e:
        CLOUD_LAST_ERROR = f"å¯«å…¥é›²ç«¯å¤±æ•—ï¼š{e}"
        return False, 0
    try:
        ws = _open_or_create_ws(client)
        ws.clear()
        set_with_dataframe(ws, df, include_index=False, include_column_header=True, resize=True)
        CLOUD_LAST_ERROR = ""
        # é‡æ–°æŠ“ä¸€æ¬¡è¡Œæ•¸
        total_rows = len(df.index)
        return True, total_rows
    except Exception as e:
        CLOUD_LAST_ERROR = f"å¯«å…¥é›²ç«¯å¤±æ•—ï¼š{e}"
        return False, 0

# ------------ æœ¬åœ° CSV å‚™æ´ ------------

def ensure_records_csv():
    if not RECORDS_CSV.exists():
        cols = ["date", "item"]
        for s in range(1, NUM_SETS+1):
            cols += [f"set{s}_kg", f"set{s}_reps"]
        cols += ["note", "total_volume_kg", "created_at"]
        pd.DataFrame(columns=cols).to_csv(RECORDS_CSV, index=False, encoding="utf-8")


def load_local_df() -> pd.DataFrame:
    ensure_records_csv()
    try:
        return pd.read_csv(RECORDS_CSV)
    except Exception:
        return pd.DataFrame()


def write_local_df(df: pd.DataFrame):
    df.to_csv(RECORDS_CSV, index=False, encoding="utf-8")


# ------------ å„ªå…ˆé›²ç«¯ ------------

def load_records_df() -> pd.DataFrame:
    df = read_cloud_df()
    if df is not None:
        return df
    return load_local_df()


def save_records_df(df: pd.DataFrame) -> Tuple[bool, int]:
    ok_cloud, total_rows = write_cloud_df(df)
    write_local_df(df)
    return ok_cloud, total_rows


# ------------ å…¶ä»–å·¥å…· ------------

def load_known_items() -> List[str]:
    if ITEMS_JSON.exists():
        try:
            return json.loads(ITEMS_JSON.read_text(encoding="utf-8"))
        except Exception:
            return []
    return []


def save_known_items(items: List[str]):
    uniq = []
    for it in items:
        it = (it or "").strip()
        if it and it not in uniq:
            uniq.append(it)
    ITEMS_JSON.write_text(json.dumps(uniq, ensure_ascii=False, indent=2), encoding="utf-8")


def get_all_item_choices() -> List[str]:
    seen: List[str] = []
    df = read_cloud_df()
    if df is not None and not df.empty and "item" in df.columns:
        counts = df["item"].dropna().astype(str).str.strip().value_counts()
        seen += [x for x in counts.index.tolist() if x]
    else:
        if RECORDS_CSV.exists():
            try:
                df_local = pd.read_csv(RECORDS_CSV)
                if "item" in df_local.columns:
                    counts = df_local["item"].dropna().astype(str).str.strip().value_counts()
                    seen += [x for x in counts.index.tolist() if x]
            except Exception:
                pass
    for it in load_known_items():
        if it and it not in seen:
            seen.append(it)
    return seen


def compute_total_volume(kg_list: List[float|None], reps_list: List[int|None]) -> float:
    total = 0.0
    for k, r in zip(kg_list, reps_list):
        if k is None or r is None:
            continue
        try:
            total += float(k) * int(r)
        except Exception:
            pass
    return round(total, 2)


def hash_entry(row: dict) -> str:
    key = json.dumps({k: row.get(k) for k in [
        "date", "item",
        *[f"set{i}_kg" for i in range(1, NUM_SETS+1)],
        *[f"set{i}_reps" for i in range(1, NUM_SETS+1)],
        "note"
    ]}, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(key.encode("utf-8")).hexdigest()


# ------------ å„²å­˜ï¼ˆå«è¦†å¯«èˆ‡é‡è¤‡åˆ¤æ–·ï¼‰ ------------

def save_button_clicked(date_str: str, item_name: str,
                        set1kg, set1reps, set2kg, set2reps, set3kg, set3reps, set4kg, set4reps, set5kg, set5reps,
                        note: str):
    # è§£ææ—¥æœŸ
    try:
        dt = pd.to_datetime(date_str).date()
    except Exception:
        return "æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ç”¨ YYYY-MM-DD", gr.update(), pd.DataFrame(), gr.update()

    item_name = (item_name or "").strip()
    if not item_name:
        return "æ²’æœ‰å¯å­˜çš„è³‡æ–™ï¼šè«‹è‡³å°‘å¡«ä¸€å€‹ Item åç¨±", gr.update(), pd.DataFrame(), gr.update()

    # è§£ææ•¸å€¼
    def to_f(x):
        return None if x in ("", None) else float(x)
    def to_i(x):
        return None if x in ("", None) else int(x)

    kg_vals = [to_f(set1kg), to_f(set2kg), to_f(set3kg), to_f(set4kg), to_f(set5kg)]
    reps_vals = [to_i(set1reps), to_i(set2reps), to_i(set3reps), to_i(set4reps), to_i(set5reps)]

    sets_kv = {}
    for idx, (kg, rp) in enumerate(zip(kg_vals, reps_vals), start=1):
        sets_kv[f"set{idx}_kg"] = kg
        sets_kv[f"set{idx}_reps"] = rp

    total_volume = compute_total_volume(kg_vals, reps_vals)
    now = datetime.now()
    new_row = {
        "date": dt.isoformat(),
        "item": item_name,
        **sets_kv,
        "note": note or "",
        "total_volume_kg": total_volume,
        "created_at": now.isoformat(timespec="seconds"),
    }
    new_hash = hash_entry(new_row)

    # è®€ç¾æœ‰
    df = load_records_df()

    # æ‰¾æœ€è¿‘åŒæ—¥+åŒ item
    idx_recent = None
    recent_row = None
    if not df.empty:
        try:
            df_tmp = df.copy()
            df_tmp["created_at_dt"] = pd.to_datetime(df_tmp.get("created_at"), errors="coerce")
            mask = (df_tmp["date"].astype(str) == new_row["date"]) & (df_tmp["item"].astype(str) == new_row["item"])
            df_same = df_tmp[mask].sort_values("created_at_dt", ascending=False)
            if not df_same.empty:
                idx_recent = df_same.index[0]
                recent_row = df.loc[idx_recent].to_dict()
        except Exception:
            pass

    if recent_row is not None and hash_entry(recent_row) == new_hash:
        merged_choices = get_all_item_choices()
        latest = load_records_df()
        if not latest.empty and "note" in latest.columns:
            cols = [c for c in latest.columns if c != "note"] + ["note"]
            latest = latest[cols]
        return ("å…§å®¹æœªè®Šæ›´ï¼šæœªå„²å­˜ã€‚", gr.update(choices=merged_choices), latest.tail(20), gr.update(interactive=False))

    replaced = False
    if recent_row is not None:
        try:
            t_recent = pd.to_datetime(recent_row.get("created_at"), errors="coerce")
            if pd.notna(t_recent) and (now - t_recent.to_pydatetime()) <= timedelta(minutes=WINDOW_MINUTES):
                df = df.drop(index=idx_recent)
                replaced = True
        except Exception:
            pass

    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

    if "note" in df.columns:
        cols = [c for c in df.columns if c != "note"] + ["note"]
        df = df[cols]

    ok_cloud, total_rows = save_records_df(df)

    msg = ("å·²è¦†å¯«æœ€è¿‘ 10 åˆ†é˜å…§çš„èˆŠç´€éŒ„ã€‚" if replaced else "å·²å„²å­˜ 1 ç­†ã€‚") + f"ï¼ˆæ—¥æœŸï¼š{dt.isoformat()}ï¼‰"
    if ok_cloud:
        msg += f"ï½œé›²ç«¯åŒæ­¥âœ…ï½œåˆ†é ï¼š{CLOUD_WS_TITLE}ï½œç¸½åˆ—æ•¸ï¼š{total_rows}"
    else:
        extra = f"ï¼ˆ{CLOUD_LAST_ERROR}ï¼‰" if CLOUD_LAST_ERROR else ""
        msg += f"ï½œé›²ç«¯åŒæ­¥âŒ {extra}"

    known = load_known_items()
    if item_name not in known:
        known.append(item_name)
        save_known_items(known)

    merged_choices = get_all_item_choices()
    latest = load_records_df()
    if not latest.empty and "note" in latest.columns:
        cols = [c for c in latest.columns if c != "note"] + ["note"]
        latest = latest[cols]

    return (msg, gr.update(choices=merged_choices), latest.tail(20), gr.update(interactive=True))


# ------------ æœå°‹ï¼ˆç›´æ¥è®€é›²ç«¯ï¼Œå¤±æ•—å‰‡å‚™æ´ï¼‰ ------------

def search_records(date_from: str, date_to: str, item_filter: str):
    df = load_records_df()

    if date_from:
        try:
            df = df[df["date"] >= pd.to_datetime(date_from).date().isoformat()]
        except Exception:
            pass
    if date_to:
        try:
            df = df[df["date"] <= pd.to_datetime(date_to).date().isoformat()]
        except Exception:
            pass

    if item_filter:
        df = df[df["item"].astype(str).str.contains(item_filter, case=False, na=False)]

    if not df.empty:
        try:
            df["created_at_dt"] = pd.to_datetime(df["created_at"], errors="coerce")
            df = df.sort_values(["date", "created_at_dt"], ascending=[False, False])
            df = df.drop(columns=["created_at_dt"], errors="ignore")
        except Exception:
            pass
        if "note" in df.columns:
            cols = [c for c in df.columns if c != "note"] + ["note"]
            df = df[cols]
    return df


# ------------ æ•™ç·´æ©Ÿå™¨äººï¼ˆä¸²æµï¼‰ ------------

def coach_chat_stream(history: list[list[str]], user_msg: str):
    msg = (user_msg or "").strip()
    if not msg:
        yield history, ""
        return

    if groq_client is None:
        bot_text = "ï¼ˆå°šæœªè¨­å®šç’°å¢ƒè®Šæ•¸ groq_keyï¼Œè«‹è¨­å®šå¾Œé‡è©¦ã€‚ï¼‰"
        history = history + [[msg, bot_text]]
        yield history, ""
        return

    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    for u, b in history:
        if u:
            messages.append({"role": "user", "content": u})
        if b:
            messages.append({"role": "assistant", "content": b})
    messages.append({"role": "user", "content": msg})

    try:
        completion = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=messages,
            temperature=0.7,
            max_completion_tokens=512,
            top_p=1,
            stream=True,
            stop=None,
        )
        bot_resp = ""
        history = history + [[msg, ""]]
        for chunk in completion:
            delta = chunk.choices[0].delta.content or ""
            if delta:
                bot_resp += delta
                history[-1][1] = bot_resp
                yield history, ""
        return
    except Exception as e:
        history = history + [[msg, f"æŠ±æ­‰ï¼ŒGroq å‘¼å«å¤±æ•—ï¼š{e}"]]
        yield history, ""


# ------------ CSSï¼šæ“´å¤§ Note æ¬„ä½å¯¬åº¦ ------------
CSS = """
#records_df table, #latest_df table { table-layout: fixed; width: 100%; }
#records_df table th:last-child, #records_df table td:last-child,
#latest_df table th:last-child, #latest_df table td:last-child { width: 48% !important; }
"""

# ------------ ä»‹é¢ ------------
with gr.Blocks(title=APP_TITLE, theme=gr.themes.Soft(), css=CSS) as demo:
    gr.Markdown("""# ğŸ‹ï¸â€â™‚ï¸ Workout Logger + ğŸ¤– ä½ çš„æ•™ç·´
å¿«é€Ÿè¨˜éŒ„é‡é‡è¨“ç·´èˆ‡æŸ¥è©¢æ­·å²ã€‚""")

    # é›²ç«¯ç‹€æ…‹æç¤ºï¼ˆåŒ…å«ç›®æ¨™åˆ†é ï¼‰
    _df_probe = read_cloud_df()
    target = CLOUD_WS_TITLE or SHEET_TITLE_ENV
    cloud_status = "å·²é€£ç·šè‡³é›²ç«¯è©¦ç®—è¡¨ âœ…" if _df_probe is not None else f"æœªé€£ç·šè‡³é›²ç«¯ï¼ˆæ”¹ç”¨æœ¬æ©Ÿå‚™æ´ï¼‰âŒ  {CLOUD_LAST_ERROR}"
    rows_info = f"ï¼Œåˆ†é ï¼š{target}ï¼Œç›®å‰åˆ—æ•¸ï¼š{len(_df_probe) if _df_probe is not None else 0}"
    gr.Markdown(f"**Cloud**ï¼š{cloud_status}{rows_info}")

    with gr.Tabs():
        # ---- Log ----
        with gr.TabItem("Log"):
            today_str = date.today().isoformat()
            date_in = gr.Textbox(value=today_str, label="Date (YYYY-MM-DD)")

            item_choices = get_all_item_choices()
            gr.Markdown("### Item")
            item_dd = gr.Dropdown(choices=item_choices, allow_custom_value=True, value=None, label="Item åç¨±")

            with gr.Row():
                set1kg = gr.Number(label="Set 1 â€” kg", precision=2, value=None, placeholder="kg")
                set1rp = gr.Number(label="Set 1 â€” reps", precision=0, value=None, placeholder="reps")
            with gr.Row():
                set2kg = gr.Number(label="Set 2 â€” kg", precision=2, value=None, placeholder="kg")
                set2rp = gr.Number(label="Set 2 â€” reps", precision=0, value=None, placeholder="reps")
            with gr.Row():
                set3kg = gr.Number(label="Set 3 â€” kg", precision=2, value=None, placeholder="kg")
                set3rp = gr.Number(label="Set 3 â€” reps", precision=0, value=None, placeholder="reps")
            with gr.Row():
                set4kg = gr.Number(label="Set 4 â€” kg", precision=2, value=None, placeholder="kg")
                set4rp = gr.Number(label="Set 4 â€” reps", precision=0, value=None, placeholder="reps")
            with gr.Row():
                set5kg = gr.Number(label="Set 5 â€” kg", precision=2, value=None, placeholder="kg")
                set5rp = gr.Number(label="Set 5 â€” reps", precision=0, value=None, placeholder="reps")

            note_in = gr.Textbox(label="Note", placeholder="RPEã€æ„Ÿè¦ºã€ä¸‹æ¬¡èª¿æ•´â€¦")

            save_btn = gr.Button("ğŸ’¾ Save", variant="primary")
            status_md = gr.Markdown("")
            current_df = load_records_df()
            latest_df = gr.Dataframe(headers=None, value=current_df.tail(20) if not current_df.empty else pd.DataFrame(),
                                     wrap=True, interactive=False, label="æœ€è¿‘ 20 ç­†ç´€éŒ„", elem_id="latest_df")

            save_btn.click(
                fn=save_button_clicked,
                inputs=[date_in, item_dd,
                        set1kg, set1rp, set2kg, set2rp, set3kg, set3rp, set4kg, set4rp, set5kg, set5rp,
                        note_in],
                outputs=[status_md, item_dd, latest_df, save_btn],
            )

        # ---- Records ----
        with gr.TabItem("Records"):
            with gr.Row():
                q_from = gr.Textbox(label="From (YYYY-MM-DD)")
                q_to = gr.Textbox(label="To (YYYY-MM-DD)")
                q_item = gr.Textbox(label="Item åŒ…å«ï¼ˆé—œéµå­—ï¼‰")
            query_btn = gr.Button("ğŸ” Search")
            out_df = gr.Dataframe(headers=None, value=load_records_df(), wrap=True, interactive=False, label="æœå°‹çµæœ", elem_id="records_df")
            query_btn.click(search_records, inputs=[q_from, q_to, q_item], outputs=out_df)

        # ---- ä½ çš„æ•™ç·´ ----
        with gr.TabItem("ä½ çš„æ•™ç·´"):
            chatbot = gr.Chatbot(height=420, type="messages")
            user_in = gr.Textbox(placeholder="è¼¸å…¥ä½ çš„å•é¡Œï¼ŒæŒ‰ Enter æˆ–é»é€å‡ºâ€¦", label="è¨Šæ¯")
            with gr.Row():
                send_btn = gr.Button("é€å‡º", variant="primary")
                clear_btn = gr.Button("æ¸…ç©º")
            send_btn.click(coach_chat_stream, inputs=[chatbot, user_in], outputs=[chatbot, user_in])
            user_in.submit(coach_chat_stream, inputs=[chatbot, user_in], outputs=[chatbot, user_in])
            clear_btn.click(lambda: ([], ""), None, [chatbot, user_in], queue=False)

    gr.Markdown("""---
**Tips**
- Item åç¨±å¯ç›´æ¥è¼¸å…¥æ–°æ–‡å­—ï¼Œä¸‹æ¬¡æœƒå‡ºç¾åœ¨ä¸‹æ‹‰é¸å–®ã€‚
- ç©ºç™½çš„æ•¸å€¼æ¬„æœƒä¿æŒç©ºç™½ï¼ˆä¸é¡¯ç¤º 0ï¼‰ã€‚
- Total Volume = âˆ‘(kg Ã— reps)ã€‚
""")

if __name__ == "__main__":
    if not RECORDS_CSV.exists():
        ensure_records_csv()
    demo.launch()
