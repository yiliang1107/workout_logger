"""
Gradio Workout Logger + ä½ çš„æ•™ç·´ï¼ˆGroqï¼‰â€” å–®æª”å¯åŸ·è¡Œ app.py
æ›´æ–°ï¼š
- æ¯æ¬¡åªè¨˜éŒ„ 1 å€‹ Itemï¼ˆå¤šæ¬¡ Save ä»¥è¿½åŠ ï¼‰
- kg / reps è¼¸å…¥é è¨­ç‚ºç©ºï¼ˆä¸é¡¯ç¤º 0ï¼‰
- Item ä¸‹æ‹‰æœƒåˆ—å‡ºã€Œé›²ç«¯ç´€éŒ„ï¼ˆGoogle Sheetï¼‰ã€èˆ‡æœ¬åœ°å·²çŸ¥å‹•ä½œ
- Save æ™‚ï¼š
  * ç›´æ¥ä»¥ Google Sheet ä½œç‚ºè³‡æ–™ä¾†æºèˆ‡é¡¯ç¤ºä¾†æºï¼ˆ<cloud record>ï¼‰
  * è‹¥å…§å®¹èˆ‡æœ€è¿‘ä¸€æ¬¡ï¼ˆåŒ itemã€åŒæ—¥æœŸï¼‰å®Œå…¨ç›¸åŒ â‡’ ä¸å„²å­˜ä¸¦åœç”¨ Save æŒ‰éˆ•
  * è‹¥ 10 åˆ†é˜å…§åŒ item åŒæ—¥æœŸæœ‰èˆŠç´€éŒ„ â‡’ ä»¥æ–°å…§å®¹è¦†è“‹ï¼ˆåˆªèˆŠå¯«æ–°ï¼‰
  * æ¯æ¬¡å­˜æª”å¾Œï¼Œæ•´è¡¨åŒæ­¥è‡³ Google Sheetï¼›æœ¬åœ° CSV åƒ…ä½œç‚ºå‚™æ´é¡åƒ
- Records / æœ€æ–°ç´€éŒ„ï¼šç›´æ¥è®€å– Google Sheetï¼›Note æ¬„ä½æœ€å¯¬

åŸ·è¡Œæ–¹å¼ï¼š
    pip install gradio pandas python-dateutil
    python app.py
æˆæ¬Šæ–¹å¼ï¼ˆGoogle Sheetï¼‰ï¼š
    å»ºè­°ä½¿ç”¨ Service Accountï¼Œä¸¦å°‡è©²å¸³æˆ¶çš„ email åˆ†äº«ç‚ºè©¦ç®—è¡¨çš„ã€Œå¯ç·¨è¼¯ã€
    1) è¨­å®šç’°å¢ƒè®Šæ•¸ `gspread_service_json` ç‚º service account JSON å…§å®¹ï¼ˆæ•´æ®µå­—ä¸²ï¼‰
       æˆ–è¨­å®š `GOOGLE_APPLICATION_CREDENTIALS` æŒ‡å‘æœ¬æ©Ÿ JSON æª”æ¡ˆ
"""
from __future__ import annotations
import os
import json
from pathlib import Path
from typing import List, Optional
from datetime import datetime, date, timedelta
import hashlib

# ä¾éœ€æ±‚ï¼šgroq å®‰è£/åŒ¯å…¥
try:
    from groq import Groq
except ImportError:
    os.system('pip install groq')
    from groq import Groq

# Google Sheets ç›¸ä¾
try:
    import gspread
    from gspread_dataframe import set_with_dataframe, get_as_dataframe
except ImportError:
    os.system('pip install gspread gspread_dataframe')
    import gspread
    from gspread_dataframe import set_with_dataframe, get_as_dataframe

import gradio as gr
import pandas as pd

# ------------ å¸¸æ•¸èˆ‡æª”æ¡ˆè·¯å¾‘ ------------
APP_TITLE = "Workout Logger"
RECORDS_CSV = Path("workout_records.csv")
ITEMS_JSON = Path("known_items.json")
NUM_ITEMS = 1            # æ¯æ¬¡åªè¨˜éŒ„ 1 å€‹ Item
NUM_SETS = 5
WINDOW_MINUTES = 10      # 10 åˆ†é˜å…§å¯è¦†å¯«
SHEET_ID = "1qWH-FQKqAMLXdN2uV4fcLIk5URRjBwY7nELznZ352og"
SHEET_TITLE = "records"

# ------------ Groqï¼ˆæ•™ç·´æ©Ÿå™¨äººï¼‰è¨­å®š ------------
GROQ_API_KEY = os.getenv("groq_key")  # ç”¨ secret: groq_key
try:
    groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
except Exception:
    groq_client = None

SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€å€‹è¬›ç¹é«”ä¸­æ–‡(Zh-tw)çš„å¥èº«æ•™ç·´ï¼Œä½ å¾ˆæ¨‚è§€ã€æœƒé¼“å‹µäººï¼Œä¹Ÿæœƒè¬›æœ‰è¶£çš„ç¬‘è©±ã€‚"
    "ç„¡è«–å­¸ç”Ÿå•ä»€éº¼å•é¡Œï¼Œéƒ½ç›¡é‡æŠŠè©±é¡Œå¼•å°è‡³é‹å‹•èˆ‡å¥èº«ã€‚è«‹ç”¨å£èªã€çŸ­æ®µè½ï¼Œ"
    "æä¾›å…·é«”å¯è¡Œçš„è¨“ç·´å»ºè­°ï¼ˆå‹•ä½œ/çµ„æ•¸/é‡é‡æˆ–RPEï¼‰ï¼Œä¸¦é©åº¦æé†’å®‰å…¨èˆ‡æš–èº«æ”¾é¬†ã€‚"
)
GROQ_MODEL = "llama-3.3-70b-versatile"

# ------------ Google Sheets å·¥å…· ------------

def _gs_client() -> Optional[gspread.Client]:
    try:
        sa_json = os.getenv("gspread_service_json") or os.getenv("GOOGLE_SERVICE_ACCOUNT_JSON")
        if sa_json:
            creds_dict = json.loads(sa_json)
            return gspread.service_account_from_dict(creds_dict)
        # å¦å‰‡èµ° GOOGLE_APPLICATION_CREDENTIALS
        return gspread.service_account()
    except Exception:
        return None


def _open_or_create_ws(client: gspread.Client):
    sh = client.open_by_key(SHEET_ID)
    try:
        ws = sh.worksheet(SHEET_TITLE)
    except gspread.WorksheetNotFound:
        ws = sh.add_worksheet(title=SHEET_TITLE, rows=1000, cols=30)
    # ç¢ºä¿è¡¨é ­
    ensure_records_header(ws)
    return ws


def ensure_records_header(ws):
    cols = ["date", "item"]
    for s in range(1, NUM_SETS+1):
        cols += [f"set{s}_kg", f"set{s}_reps"]
    cols += ["note", "total_volume_kg", "created_at"]
    try:
        first_row = ws.row_values(1)
    except Exception:
        first_row = []
    if first_row != cols:
        ws.clear()
        ws.update([cols])


def read_cloud_df() -> Optional[pd.DataFrame]:
    client = _gs_client()
    if not client:
        return None
    try:
        ws = _open_or_create_ws(client)
        df = get_as_dataframe(ws, evaluate_formulas=True, header=0)
        # ç§»é™¤å…¨ç©ºåˆ—ï¼›ç¢ºä¿æ¬„ä½å
        df = df.dropna(how='all')
        if df.empty:
            # å»ºç«‹ç©º DataFrame ä½†æœ‰æ­£ç¢ºæ¬„ä½
            cols = ["date", "item"]
            for s in range(1, NUM_SETS+1):
                cols += [f"set{s}_kg", f"set{s}_reps"]
            cols += ["note", "total_volume_kg", "created_at"]
            df = pd.DataFrame(columns=cols)
        # è½‰å­—ä¸²å‹æ…‹ä»¥é¿å… NaN å•é¡Œï¼ˆé™¤æ•¸å€¼æ¬„ï¼‰
        return df
    except Exception:
        return None


def write_cloud_df(df: pd.DataFrame) -> bool:
    client = _gs_client()
    if not client:
        return False
    try:
        ws = _open_or_create_ws(client)
        ws.clear()
        set_with_dataframe(ws, df, include_index=False, include_column_header=True, resize=True)
        return True
    except Exception:
        return False

# ------------ æœ¬åœ° CSV å‚™æ´ ------------

def ensure_records_csv():
    if not RECORDS_CSV.exists():
        cols = ["date", "item"]
        for s in range(1, NUM_SETS+1):
            cols += [f"set{s}_kg", f"set{s}_reps"]
        cols += ["note", "total_volume_kg", "created_at"]
        pd.DataFrame(columns=cols).to_csv(RECORDS_CSV, index=False, encoding="utf-8")


def load_local_df() -> pd.DataFrame:
    ensure_records_csv()
    try:
        return pd.read_csv(RECORDS_CSV)
    except Exception:
        return pd.DataFrame()


def write_local_df(df: pd.DataFrame):
    df.to_csv(RECORDS_CSV, index=False, encoding="utf-8")


# ------------ è¼”åŠ©ï¼šä¾†æºå„ªå…ˆé›²ç«¯ ------------

def load_records_df() -> pd.DataFrame:
    df = read_cloud_df()
    if df is not None:
        return df
    return load_local_df()


def save_records_df(df: pd.DataFrame) -> bool:
    ok_cloud = write_cloud_df(df)
    write_local_df(df)
    return ok_cloud


# ------------ å…¶ä»–å·¥å…· ------------

def load_known_items() -> List[str]:
    if ITEMS_JSON.exists():
        try:
            return json.loads(ITEMS_JSON.read_text(encoding="utf-8"))
        except Exception:
            return []
    return []


def save_known_items(items: List[str]):
    uniq = []
    for it in items:
        it = (it or "").strip()
        if it and it not in uniq:
            uniq.append(it)
    ITEMS_JSON.write_text(json.dumps(uniq, ensure_ascii=False, indent=2), encoding="utf-8")


def get_all_item_choices() -> List[str]:
    seen: List[str] = []
    # å¾é›²ç«¯è®€
    df = read_cloud_df()
    if df is not None and not df.empty and "item" in df.columns:
        counts = df["item"].dropna().astype(str).str.strip().value_counts()
        seen += [x for x in counts.index.tolist() if x]
    else:
        # å¾æœ¬åœ°å‚™æ´
        if RECORDS_CSV.exists():
            try:
                df_local = pd.read_csv(RECORDS_CSV)
                if "item" in df_local.columns:
                    counts = df_local["item"].dropna().astype(str).str.strip().value_counts()
                    seen += [x for x in counts.index.tolist() if x]
            except Exception:
                pass
    # åŠ ä¸Š JSON known
    for it in load_known_items():
        if it and it not in seen:
            seen.append(it)
    return seen


def compute_total_volume(kg_list: List[float|None], reps_list: List[int|None]) -> float:
    total = 0.0
    for k, r in zip(kg_list, reps_list):
        if k is None or r is None:
            continue
        try:
            total += float(k) * int(r)
        except Exception:
            pass
    return round(total, 2)


def hash_entry(row: dict) -> str:
    key = json.dumps({k: row.get(k) for k in [
        "date", "item",
        *[f"set{i}_kg" for i in range(1, NUM_SETS+1)],
        *[f"set{i}_reps" for i in range(1, NUM_SETS+1)],
        "note"
    ]}, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(key.encode("utf-8")).hexdigest()


# ------------ å„²å­˜å‹•ä½œï¼ˆå« 10 åˆ†é˜è¦†å¯« & é‡è¤‡æª¢æŸ¥ï¼‰ ------------

def save_button_clicked(date_str: str, *flat_inputs):
    # è§£ææ—¥æœŸ
    try:
        dt = pd.to_datetime(date_str).date()
    except Exception:
        return "æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ç”¨ YYYY-MM-DD", gr.update(), pd.DataFrame(), gr.update()

    # å±•å¹³ï¼šå–®ä¸€ item
    block_size = 1 + (NUM_SETS * 2) + 1
    chunk = list(flat_inputs[:block_size])
    item_name = (chunk[0] or "").strip()
    if not item_name:
        return "æ²’æœ‰å¯å­˜çš„è³‡æ–™ï¼šè«‹è‡³å°‘å¡«ä¸€å€‹ Item åç¨±", gr.update(), pd.DataFrame(), gr.update()

    kg_vals, reps_vals = [], []
    sets_kv = {}
    pos = 1
    for s in range(1, NUM_SETS+1):
        kg = chunk[pos]; reps = chunk[pos+1]
        pos += 2
        kg = None if kg in ("", None) else float(kg)
        reps = None if reps in ("", None) else int(reps)
        sets_kv[f"set{s}_kg"] = kg
        sets_kv[f"set{s}_reps"] = reps
        kg_vals.append(kg)
        reps_vals.append(reps)
    note = chunk[pos] if pos < len(chunk) else ""

    total_volume = compute_total_volume(kg_vals, reps_vals)
    now = datetime.now()
    new_row = {
        "date": dt.isoformat(),
        "item": item_name,
        **sets_kv,
        "note": note,
        "total_volume_kg": total_volume,
        "created_at": now.isoformat(timespec="seconds"),
    }
    new_hash = hash_entry(new_row)

    # è¼‰å…¥ä¾†æºï¼ˆå„ªå…ˆé›²ç«¯ï¼‰
    df = load_records_df()

    # ç¯©åŒæ—¥åŒ item æœ€è¿‘ä¸€ç­†
    idx_recent = None
    recent_row = None
    if not df.empty:
        try:
            df_tmp = df.copy()
            if "created_at" in df_tmp.columns:
                df_tmp["created_at_dt"] = pd.to_datetime(df_tmp["created_at"], errors="coerce")
            else:
                df_tmp["created_at_dt"] = pd.NaT
            mask = (df_tmp["date"].astype(str) == new_row["date"]) & (df_tmp["item"].astype(str) == new_row["item"])
            df_same = df_tmp[mask].sort_values("created_at_dt", ascending=False)
            if not df_same.empty:
                idx_recent = df_same.index[0]
                recent_row = df.loc[idx_recent].to_dict()
        except Exception:
            pass

    # è‹¥å…§å®¹æœªè®Šæ›´ï¼šä¸å„²å­˜ï¼Œä¸¦åœç”¨ Save
    if recent_row is not None:
        recent_hash = hash_entry(recent_row)
        if recent_hash == new_hash:
            merged_choices = get_all_item_choices()
            latest = load_records_df()
            if not latest.empty and "note" in latest.columns:
                cols = [c for c in latest.columns if c != "note"] + ["note"]
                latest = latest[cols]
            return ("å…§å®¹æœªè®Šæ›´ï¼šæœªå„²å­˜ã€‚", gr.update(choices=merged_choices), latest.tail(20), gr.update(interactive=False))

    # è‹¥ 10 åˆ†é˜å…§æœ‰èˆŠç´€éŒ„ï¼šè¦†å¯«ï¼ˆåˆªèˆŠå¯«æ–°ï¼‰
    replaced = False
    if recent_row is not None:
        try:
            t_recent = pd.to_datetime(recent_row.get("created_at"), errors="coerce")
            if pd.notna(t_recent) and (now - t_recent.to_pydatetime()) <= timedelta(minutes=WINDOW_MINUTES):
                # åˆªé™¤èˆŠè¡Œ
                df = df.drop(index=idx_recent)
                replaced = True
        except Exception:
            pass

    # è¿½åŠ æ–°è¡Œ
    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

    # è®“ note æ”¾æœ€å¾Œä¸€æ¬„
    if "note" in df.columns:
        cols = [c for c in df.columns if c != "note"] + ["note"]
        df = df[cols]

    # åŒæ­¥å¯«å›é›²ç«¯èˆ‡æœ¬åœ°
    save_records_df(df)

    # æ›´æ–°å·²çŸ¥ item æ¸…å–®
    known = load_known_items()
    if item_name not in known:
        known.append(item_name)
        save_known_items(known)

    merged_choices = get_all_item_choices()
    latest = load_records_df()
    if not latest.empty and "note" in latest.columns:
        cols = [c for c in latest.columns if c != "note"] + ["note"]
        latest = latest[cols]

    msg = ("å·²è¦†å¯«æœ€è¿‘ 10 åˆ†é˜å…§çš„èˆŠç´€éŒ„ã€‚" if replaced else "å·²å„²å­˜ 1 ç­†ã€‚") + f"ï¼ˆæ—¥æœŸï¼š{dt.isoformat()}ï¼‰"
    return (msg, gr.update(choices=merged_choices), latest.tail(20), gr.update(interactive=True))


# ------------ Records æœå°‹ï¼ˆç›´æ¥è®€é›²ç«¯ï¼Œå¤±æ•—å‰‡å‚™æ´ï¼‰ ------------

def search_records(date_from: str, date_to: str, item_filter: str):
    df = load_records_df()

    if date_from:
        try:
            df = df[df["date"] >= pd.to_datetime(date_from).date().isoformat()]
        except Exception:
            pass
    if date_to:
        try:
            df = df[df["date"] <= pd.to_datetime(date_to).date().isoformat()]
        except Exception:
            pass

    if item_filter:
        df = df[df["item"].astype(str).str.contains(item_filter, case=False, na=False)]

    if not df.empty:
        # created_at ç”±æ–°åˆ°èˆŠ
        try:
            df["created_at_dt"] = pd.to_datetime(df["created_at"], errors="coerce")
            df = df.sort_values(["date", "created_at_dt"], ascending=[False, False])
            df = df.drop(columns=["created_at_dt"], errors="ignore")
        except Exception:
            pass
        # è®“ note æ”¾æœ€å¾Œä¸€æ¬„
        if "note" in df.columns:
            cols = [c for c in df.columns if c != "note"] + ["note"]
            df = df[cols]
    return df


# ------------ æ•™ç·´æ©Ÿå™¨äººï¼šä¸²æµå›è¦† ------------

def coach_chat_stream(history: list[list[str]], user_msg: str):
    msg = (user_msg or "").strip()
    if not msg:
        yield history, ""
        return

    if groq_client is None:
        bot_text = "ï¼ˆå°šæœªè¨­å®šç’°å¢ƒè®Šæ•¸ groq_keyï¼Œè«‹è¨­å®šå¾Œé‡è©¦ã€‚ï¼‰"
        history = history + [[msg, bot_text]]
        yield history, ""
        return

    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    for u, b in history:
        if u:
            messages.append({"role": "user", "content": u})
        if b:
            messages.append({"role": "assistant", "content": b})
    messages.append({"role": "user", "content": msg})

    try:
        completion = groq_client.chat.completions.create(
            model=GROQ_MODEL,
            messages=messages,
            temperature=0.7,
            max_completion_tokens=512,
            top_p=1,
            stream=True,
            stop=None,
        )
        bot_resp = ""
        history = history + [[msg, ""]]
        for chunk in completion:
            delta = chunk.choices[0].delta.content or ""
            if delta:
                bot_resp += delta
                history[-1][1] = bot_resp
                yield history, ""
        return
    except Exception as e:
        history = history + [[msg, f"æŠ±æ­‰ï¼ŒGroq å‘¼å«å¤±æ•—ï¼š{e}"]]
        yield history, ""


# ------------ CSSï¼šæ“´å¤§ Note æ¬„ä½å¯¬åº¦ ------------
CSS = """
#records_df table, #latest_df table { table-layout: fixed; width: 100%; }
#records_df table th:last-child, #records_df table td:last-child,
#latest_df table th:last-child, #latest_df table td:last-child { width: 48% !important; }
"""

# ------------ å»ºç«‹ä»‹é¢ ------------
with gr.Blocks(title=APP_TITLE, theme=gr.themes.Soft(), css=CSS) as demo:
    gr.Markdown("""# ğŸ‹ï¸â€â™‚ï¸ Workout Logger + ğŸ¤– ä½ çš„æ•™ç·´
å¿«é€Ÿè¨˜éŒ„é‡é‡è¨“ç·´èˆ‡æŸ¥è©¢æ­·å²ã€‚""")

    with gr.Tabs():
        # ---- Log åˆ†é ï¼ˆå–®ä¸€ Itemï¼‰ ----
        with gr.TabItem("Log"):
            today_str = date.today().isoformat()
            date_in = gr.Textbox(value=today_str, label="Date (YYYY-MM-DD)")

            # é¸å–®ï¼šåˆä½µé›²ç«¯èˆ‡å·²çŸ¥
            item_choices = get_all_item_choices()

            gr.Markdown("### Item 1")
            item_dd = gr.Dropdown(choices=item_choices, allow_custom_value=True, value=None, label="Item åç¨±")

            set_inputs = []
            for s in range(1, NUM_SETS+1):
                with gr.Row():
                    kg = gr.Number(label=f"Set {s} â€” kg", precision=2, value=None, placeholder="kg")
                    reps = gr.Number(label=f"Set {s} â€” reps", precision=0, value=None, placeholder="reps")
                    set_inputs += [kg, reps]
            note_in = gr.Textbox(label="Note", placeholder="RPEã€æ„Ÿè¦ºã€ä¸‹æ¬¡èª¿æ•´â€¦")

            save_btn = gr.Button("ğŸ’¾ Save", variant="primary")
            status_md = gr.Markdown("")
            latest_df = gr.Dataframe(headers=None, value=load_records_df().tail(20) if not load_records_df().empty else pd.DataFrame(),
                                     wrap=True, interactive=False, label="æœ€è¿‘ 20 ç­†ç´€éŒ„", elem_id="latest_df")

            flat_inputs = [item_dd, *set_inputs, note_in]
            save_btn.click(
                fn=save_button_clicked,
                inputs=[date_in, *flat_inputs],
                outputs=[status_md, item_dd, latest_df, save_btn],
            )

        # ---- Records åˆ†é  ----
        with gr.TabItem("Records"):
            with gr.Row():
                q_from = gr.Textbox(label="From (YYYY-MM-DD)")
                q_to = gr.Textbox(label="To (YYYY-MM-DD)")
                q_item = gr.Textbox(label="Item åŒ…å«ï¼ˆé—œéµå­—ï¼‰")
            query_btn = gr.Button("ğŸ” Search")
            out_df = gr.Dataframe(headers=None, value=load_records_df(), wrap=True, interactive=False, label="æœå°‹çµæœ", elem_id="records_df")
            query_btn.click(search_records, inputs=[q_from, q_to, q_item], outputs=out_df)

        # ---- ä½ çš„æ•™ç·´ï¼ˆç„¡èªªæ˜æ–‡å­—ï¼‰ ----
        with gr.TabItem("ä½ çš„æ•™ç·´"):
            chatbot = gr.Chatbot(height=420)
            user_in = gr.Textbox(placeholder="è¼¸å…¥ä½ çš„å•é¡Œï¼ŒæŒ‰ Enter æˆ–é»é€å‡ºâ€¦", label="è¨Šæ¯")
            with gr.Row():
                send_btn = gr.Button("é€å‡º", variant="primary")
                clear_btn = gr.Button("æ¸…ç©º")
            send_btn.click(coach_chat_stream, inputs=[chatbot, user_in], outputs=[chatbot, user_in])
            user_in.submit(coach_chat_stream, inputs=[chatbot, user_in], outputs=[chatbot, user_in])
            clear_btn.click(lambda: ([], ""), None, [chatbot, user_in], queue=False)

    gr.Markdown("""---
**Tips**
- Item åç¨±å¯ç›´æ¥è¼¸å…¥æ–°æ–‡å­—ï¼Œä¸‹æ¬¡æœƒå‡ºç¾åœ¨ä¸‹æ‹‰é¸å–®ã€‚
- ç©ºç™½çš„æ•¸å€¼æ¬„æœƒä¿æŒç©ºç™½ï¼ˆä¸é¡¯ç¤º 0ï¼‰ã€‚
- Total Volume = âˆ‘(kg Ã— reps)ã€‚
""")

if __name__ == "__main__":
    # å»ºç«‹æœ¬åœ°å‚™æ´æª”
    if not RECORDS_CSV.exists():
        ensure_records_csv()
    demo.launch()
